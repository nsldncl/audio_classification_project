{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2AngCgD-MyF"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchlibrosa dotmap loguru warmup_scheduler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUyCddnP-_Mb",
        "outputId": "a5e6f823-cd96-437c-8353-4caff8d34f85"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchlibrosa in /usr/local/lib/python3.7/dist-packages (0.0.9)\n",
            "Collecting dotmap\n",
            "  Downloading dotmap-1.3.26-py3-none-any.whl (11 kB)\n",
            "Collecting loguru\n",
            "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting warmup_scheduler\n",
            "  Downloading warmup_scheduler-0.3.tar.gz (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchlibrosa) (1.19.5)\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from torchlibrosa) (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa) (21.3)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa) (0.51.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa) (1.1.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa) (1.5.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torchlibrosa) (0.10.3.post1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torchlibrosa) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torchlibrosa) (57.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa>=0.6.0->torchlibrosa) (3.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.0->torchlibrosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.0->torchlibrosa) (1.4.4)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa>=0.6.0->torchlibrosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.0->torchlibrosa) (3.0.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.6.0->torchlibrosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torchlibrosa) (2.21)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.6.0->torchlibrosa) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.6.0->torchlibrosa) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.6.0->torchlibrosa) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.6.0->torchlibrosa) (3.0.4)\n",
            "Building wheels for collected packages: warmup-scheduler\n",
            "  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3-py3-none-any.whl size=2981 sha256=c88a38d83cb5b96d0b2b179b245599b3fa2f744b72be6ca506243b17870ca20b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/32/98/174058864084d31dbef4c4faa502a1fe9c12cc47e9cb6611e4\n",
            "Successfully built warmup-scheduler\n",
            "Installing collected packages: warmup-scheduler, loguru, dotmap\n",
            "Successfully installed dotmap-1.3.26 loguru-0.5.3 warmup-scheduler-0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fKOYluYR-MyS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer,TransformerDecoder, TransformerDecoderLayer\n",
        "\n",
        "import csv\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "from dotmap import DotMap\n",
        "import time\n",
        "import sys\n",
        "from loguru import logger\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import math\n",
        "from pprint import PrettyPrinter\n",
        "from warmup_scheduler import GradualWarmupScheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for IO operations"
      ],
      "metadata": {
        "id": "sOELonNO-udj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "loMgbs50-MyW"
      },
      "outputs": [],
      "source": [
        "def write_csv_file(csv_obj, file_name):\n",
        "    with open(file_name, 'w') as f:\n",
        "        writer = csv.DictWriter(f, csv_obj[0].keys())\n",
        "        writer.writeheader()\n",
        "        writer.writerows(csv_obj)\n",
        "    print(f'Write to {file_name} successfully.')\n",
        "\n",
        "def load_csv_file(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        csv_reader = csv.DictReader(f)\n",
        "        csv_obj = [csv_line for csv_line in csv_reader]\n",
        "    return csv_obj\n",
        "\n",
        "def load_picke_file(file_name):\n",
        "    with open(file_name, 'rb') as f:\n",
        "        pickle_obj = pickle.load(f)\n",
        "    return pickle_obj\n",
        "\n",
        "def write_pickle_file(obj, file_name):\n",
        "    with open(file_name, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "    print(f'Write to {file_name} successfully.')\n",
        "\n",
        "def get_config():\n",
        "    with open('settings.yaml', 'r') as f:\n",
        "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    config = DotMap(config)\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader for pytorch"
      ],
      "metadata": {
        "id": "cK6UV4Jo_VEL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RK1iakUy-MyY"
      },
      "outputs": [],
      "source": [
        "#Creating dataset class\n",
        "class ClothoDataset(Dataset):\n",
        "    def __init__(self, split,\n",
        "                 input_field_name,\n",
        "                 load_into_memory):\n",
        "        super(ClothoDataset, self).__init__()\n",
        "        split_dir = Path('data/data_splits', split)\n",
        "        self.examples = sorted(split_dir.iterdir())\n",
        "        self.input_field_name = input_field_name\n",
        "        self.output_field_name = 'words_indexs'\n",
        "        self.load_into_memory = load_into_memory\n",
        "        if load_into_memory:\n",
        "            self.examples = [np.load(str(file), allow_pickle=True) for file in self.examples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.examples[index]\n",
        "        if not self.load_into_memory:\n",
        "            item = np.load(str(item), allow_pickle=True)\n",
        "        feature = item[self.input_field_name].item()  # waveform or log melspectorgram\n",
        "        words_indexs = item[self.output_field_name].item()\n",
        "        file_name = str(item['file_name'].item())\n",
        "        caption_len = len(words_indexs)\n",
        "        caption = str(item['caption'].item())\n",
        "        return feature, words_indexs, file_name, caption_len, caption\n",
        "\n",
        "#Dataloader function this will use the above class\n",
        "#dataloader means converting data in batches\n",
        "def get_clotho_loader(split,\n",
        "                      input_field_name,\n",
        "                      load_into_memory,\n",
        "                      batch_size,\n",
        "                      shuffle=False,\n",
        "                      drop_last=False,\n",
        "                      num_workers=1):\n",
        "    dataset = ClothoDataset(split, input_field_name, load_into_memory)\n",
        "    if input_field_name == 'audio_data':\n",
        "        return DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                          shuffle=shuffle, drop_last=drop_last,\n",
        "                          num_workers=num_workers, collate_fn=clotho_collate_fn_audio)\n",
        "    else:\n",
        "        return DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                          shuffle=shuffle, drop_last=drop_last,\n",
        "                          num_workers=num_workers, collate_fn=clotho_collate_fn)\n",
        "\n",
        "#converting npy file into batches\n",
        "def clotho_collate_fn(batch):\n",
        "    max_feature_time_steps = max(i[0].shape[0] for i in batch)\n",
        "    max_caption_length = max(i[1].shape[0] for i in batch)\n",
        "    feature_number = batch[0][0].shape[-1]\n",
        "    eos_token = batch[0][1][-1]\n",
        "    feature_tensor, words_tensor = [], []\n",
        "    for feature, words_indexs, _, _, _ in batch:\n",
        "        if max_feature_time_steps > feature.shape[0]:\n",
        "            padding = torch.zeros(max_feature_time_steps - feature.shape[0], feature_number).float()\n",
        "            data = [torch.from_numpy(feature).float()]\n",
        "            data.append(padding)\n",
        "            temp_feature = torch.cat(data)\n",
        "        else:\n",
        "            temp_feature = torch.from_numpy(feature[:max_feature_time_steps, :]).float()\n",
        "        feature_tensor.append(temp_feature.unsqueeze_(0))\n",
        "\n",
        "        if max_caption_length > words_indexs.shape[0]:\n",
        "            padding = torch.ones(max_caption_length - len(words_indexs)).mul(eos_token).long()\n",
        "            data = [torch.from_numpy(words_indexs).long()]\n",
        "            data.append(padding)\n",
        "            tmp_words_indexs = torch.cat(data)\n",
        "        else:\n",
        "            tmp_words_indexs = torch.from_numpy(words_indexs[:max_caption_length]).long()\n",
        "        words_tensor.append(tmp_words_indexs.unsqueeze_(0))\n",
        "    feature_tensor = torch.cat(feature_tensor)\n",
        "    words_tensor = torch.cat(words_tensor)\n",
        "    file_names = [i[2] for i in batch]\n",
        "    caption_lens = [i[3] for i in batch]\n",
        "    captions = [i[4] for i in batch]\n",
        "    return feature_tensor, words_tensor, file_names, caption_lens, captions\n",
        "\n",
        "#converting audio files into batches\n",
        "def clotho_collate_fn_audio(batch):\n",
        "    max_audio_time_steps = max(i[0].shape[0] for i in batch)\n",
        "    max_caption_length = max(i[1].shape[0] for i in batch)\n",
        "    eos_token = batch[0][1][-1]\n",
        "    audio_tensor, words_tensor = [], []\n",
        "    for audio, words_indexs, _, _, _ in batch:\n",
        "        if max_audio_time_steps >= audio.shape[0]:\n",
        "            padding = torch.zeros(max_audio_time_steps - audio.shape[0]).float()\n",
        "            data = [torch.from_numpy(audio).float()]\n",
        "            data.append(padding)\n",
        "            temp_audio = torch.cat(data)\n",
        "        else:\n",
        "            temp_audio = torch.from_numpy(audio[:max_audio_time_steps]).float()\n",
        "        audio_tensor.append(temp_audio.unsqueeze_(0))\n",
        "        if max_caption_length >= words_indexs.shape[0]:\n",
        "            padding = torch.ones(max_caption_length - len(words_indexs)).mul(eos_token).long()\n",
        "            data = [torch.from_numpy(words_indexs).long()]\n",
        "            data.append(padding)\n",
        "            tmp_words_indexs = torch.cat(data)\n",
        "        else:\n",
        "            tmp_words_indexs = torch.from_numpy(words_indexs[:max_caption_length]).long()\n",
        "        words_tensor.append(tmp_words_indexs.unsqueeze_(0))\n",
        "    audio_tensor = torch.cat(audio_tensor)\n",
        "    words_tensor = torch.cat(words_tensor)\n",
        "    file_names = [i[2] for i in batch]\n",
        "    caption_lens = [i[3] for i in batch]\n",
        "    captions = [i[4] for i in batch]\n",
        "    return audio_tensor, words_tensor, file_names, caption_lens, captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IP73x0b-Myi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caption processing functions"
      ],
      "metadata": {
        "id": "uuetyY5yAlCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fJtkQ59O-Myj"
      },
      "outputs": [],
      "source": [
        "#just for printing logger data progress\n",
        "def rotation_logger(x, y):\n",
        "    \"\"\"Callable to determine the rotation of files in logger.\n",
        "    :param x: Str to be logged.\n",
        "    :type x: loguru._handler.StrRecord\n",
        "    :param y: File used for logging.\n",
        "    :type y: _io.TextIOWrapper\n",
        "    :return: Shall we switch to a new file?\n",
        "    :rtype: bool\n",
        "    \"\"\"\n",
        "    return 'Captions start' in x\n",
        "\n",
        "#masking the tokens\n",
        "def set_tgt_padding_mask(tgt, tgt_len):\n",
        "    batch_size = tgt.shape[0]\n",
        "    max_len = tgt.shape[1]\n",
        "    mask = torch.zeros(tgt.shape).type_as(tgt).to(tgt.device)\n",
        "    for i in range(batch_size):\n",
        "        num_pad = max_len - tgt_len[i]\n",
        "        mask[i][max_len - num_pad:] = 1\n",
        "    mask = mask.float().masked_fill(mask == 1, True).masked_fill(mask == 0, False).bool()\n",
        "    return mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN10 Encoder"
      ],
      "metadata": {
        "id": "xK3YGcAaBIzT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jY5Gbf-m-Myk"
      },
      "outputs": [],
      "source": [
        "#done in previous session\n",
        "def init_layer(layer):\n",
        "    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n",
        "    nn.init.xavier_uniform_(layer.weight)\n",
        "    if hasattr(layer, 'bias'):\n",
        "        if layer.bias is not None:\n",
        "            layer.bias.data.fill_(0.)\n",
        "            \n",
        "def init_bn(bn):\n",
        "    \"\"\"Initialize a BatchNorm layer.\"\"\"\n",
        "    bn.bias.data.fill_(0.)\n",
        "    bn.weight.data.fill_(1.)\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
        "                               out_channels=out_channels,\n",
        "                               kernel_size=(3, 3),\n",
        "                               stride=(1, 1),\n",
        "                               padding=(1, 1),\n",
        "                               bias=False)\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels,\n",
        "                               out_channels=out_channels,\n",
        "                               kernel_size=(3, 3),\n",
        "                               stride=(1, 1),\n",
        "                               padding=(1, 1),\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_layer(self.conv1)\n",
        "        init_layer(self.conv2)\n",
        "        init_bn(self.bn1)\n",
        "        init_bn(self.bn2)\n",
        "\n",
        "    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n",
        "        x = input\n",
        "        x = F.relu_(self.bn1(self.conv1(x)))\n",
        "        x = F.relu_(self.bn2(self.conv2(x)))\n",
        "        if pool_type == 'max':\n",
        "            x = F.max_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == 'avg':\n",
        "            x = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "        elif pool_type == 'avg+max':\n",
        "            x1 = F.max_pool2d(x, kernel_size=pool_size)\n",
        "            x2 = F.avg_pool2d(x, kernel_size=pool_size)\n",
        "            x = x1 + x2\n",
        "        else:\n",
        "            raise Exception('Incorrect argument!')\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "afsl2iQB-Mym"
      },
      "outputs": [],
      "source": [
        "class Cnn10(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Cnn10, self).__init__()\n",
        "        self.input_data = config.data.input_field_name\n",
        "        self.bn0 = nn.BatchNorm2d(64)\n",
        "        if self.input_data == 'audio_data':\n",
        "            sr = config.wave.sr\n",
        "            window_size = config.wave.window_size\n",
        "            hop_length = config.wave.hop_length\n",
        "            mel_bins = config.wave.mel_bins\n",
        "            fmin = config.wave.fmin\n",
        "            fmax = config.wave.fmax\n",
        "            self.spectrogram_extractor = Spectrogram(n_fft=window_size,\n",
        "                                                     hop_length=hop_length,\n",
        "                                                     win_length=window_size,\n",
        "                                                     window='hann',\n",
        "                                                     center=True,\n",
        "                                                     pad_mode='reflect',\n",
        "                                                     freeze_parameters=True)\n",
        "            self.logmel_extractor = LogmelFilterBank(sr=sr, n_fft=window_size,\n",
        "                                                     n_mels=mel_bins,\n",
        "                                                     fmin=fmin,\n",
        "                                                     fmax=fmax,\n",
        "                                                     ref=1.0,\n",
        "                                                     amin=1e-10,\n",
        "                                                     top_db=None,\n",
        "                                                     freeze_parameters=True)\n",
        "        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n",
        "        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n",
        "        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n",
        "        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n",
        "        self.fc1 = nn.Linear(512, 512, bias=True)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_bn(self.bn0)\n",
        "        init_layer(self.fc1)\n",
        "\n",
        "    def forward(self, input, mixup_param=None):\n",
        "        \"\"\" input: (batch_size, time_steps, mel_bins)\"\"\"\n",
        "        if self.input_data == 'audio_data':\n",
        "            x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n",
        "            x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n",
        "        else:\n",
        "            x = input.unsqueeze(1)  # (batch_size, 1, time_steps, mel_bins)\n",
        "        if mixup_param is not None:\n",
        "            lam, index = mixup_param\n",
        "            x = lam * x + (1 - lam) * x[index]\n",
        "        x = x.transpose(1, 3)\n",
        "        x = self.bn0(x)\n",
        "        x = x.transpose(1, 3)\n",
        "        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = torch.mean(x, dim=3)  # average in the frequency domain (batch_size, channel, time)\n",
        "        x = x.permute(2, 0, 1)  # time x batch x channel (512)\n",
        "        x = F.relu_(self.fc1(x))\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model for audio captioning"
      ],
      "metadata": {
        "id": "QHl0c1kwBcoo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bjhLm7FU-Myo"
      },
      "outputs": [],
      "source": [
        "def init_layer(layer):\n",
        "  \"\"\" Initialize a Linear or Convolutional layer. \"\"\"\n",
        "  nn.init.xavier_uniform_(layer.weight)\n",
        "  if hasattr(layer, 'bias'):\n",
        "      if layer.bias is not None:\n",
        "          layer.bias.data.fill_(0.)\n",
        "\n",
        "#to linearize the audio data(converting audio matrix to 512 size linear array-1d)\n",
        "class AudioLinear(nn.Module):\n",
        "  def __init__(self, nhid):\n",
        "      super(AudioLinear, self).__init__()\n",
        "      # self.fc1 = nn.Linear(512, 512, bias=True)\n",
        "      self.fc2 = nn.Linear(512, nhid, bias=True)\n",
        "      self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "      # init_layer(self.fc1)\n",
        "      init_layer(self.fc2)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = F.relu_(self.fc2(x))  # time x batch x nhid\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vkzvfinf-Myq"
      },
      "outputs": [],
      "source": [
        "#In this transfomer all the data are pass directly so we need to mention the position of each tokens.\n",
        "#Transformer is multi headed \n",
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
        "        in the sequence. The positional encodings have the same dimension as\n",
        "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
        "        functions of different frequencies.\n",
        "    .. math::\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=2000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#transformer model begins here.........................................................................\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\" Container module with an Cnn encoder and a Transformer decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, config, words_list, pretrained_cnn=None):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_type = 'Cnn+Transformer'\n",
        "        ntoken = len(words_list)\n",
        "        # setting for CNN\n",
        "        self.feature_extractor = Cnn10(config)\n",
        "        #if our model is not pretrained than remove external data information\n",
        "        if pretrained_cnn is not None:\n",
        "            final = pretrained_cnn[\"model\"]\n",
        "            final.pop(\"fc_audioset.weight\") \n",
        "            final.pop(\"fc_audioset.bias\")\n",
        "            self.feature_extractor.load_state_dict(final)\n",
        "        #do not train if pretrained model\n",
        "        if config.encoder.freeze:\n",
        "            for name, p in self.feature_extractor.named_parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # decoder settings\n",
        "        self.decoder_only = config.decoder.decoder_only\n",
        "        nhead = config.decoder.nhead            # number of heads in Transformer\n",
        "        self.nhid = config.decoder.nhid         # number of expected features in decoder inputs\n",
        "        nlayers = config.decoder.nlayers        # number of sub-decoder-layer in the decoder\n",
        "        dim_feedforward = config.decoder.dim_feedforward   # dimension of the feedforward model\n",
        "        activation = config.decoder.activation  # activation function of decoder intermediate layer\n",
        "        dropout = config.decoder.dropout        # the dropout value\n",
        "\n",
        "        #define the positional and audio linear encoder\n",
        "        self.pos_encoder = PositionalEncoding(self.nhid, dropout)\n",
        "        self.audio_linear = AudioLinear(self.nhid)\n",
        "  \n",
        "        ''' Including transfomer encoder '''\n",
        "        encoder_layers = TransformerEncoderLayer(self.nhid,\n",
        "                                                  nhead,\n",
        "                                                  dim_feedforward,\n",
        "                                                  dropout,\n",
        "                                                  activation)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "\n",
        "        ''' Including Transformer Decoder '''\n",
        "        decoder_layers = TransformerDecoderLayer(self.nhid,\n",
        "                                                 nhead,\n",
        "                                                 dim_feedforward,\n",
        "                                                 dropout,\n",
        "                                                 activation)\n",
        "        self.transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
        "\n",
        "\n",
        "        self.dec_fc = nn.Linear(self.nhid, ntoken)\n",
        "        self.generator = nn.Softmax(dim=-1)\n",
        "        self.word_emb = nn.Embedding(ntoken, self.nhid)\n",
        "        self.init_weights()\n",
        "        if config.word_embedding.freeze:\n",
        "            self.word_emb.weight.requires_grad = False\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.word_emb.weight.data.uniform_(-initrange, initrange)\n",
        "        self.dec_fc.bias.data.zero_()\n",
        "        self.dec_fc.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    #complete encoder of transformer\n",
        "    def encode(self, src, mixup_param=None):\n",
        "        #CNN10 is our feature extractor\n",
        "        src = self.feature_extractor(src, mixup_param)\n",
        "        #linear audio data\n",
        "        src = self.audio_linear(src)\n",
        "        src = src * math.sqrt(self.nhid)\n",
        "        #positional encoder\n",
        "        src = self.pos_encoder(src)\n",
        "        #transformer encoder\n",
        "        src = self.transformer_encoder(src, None)\n",
        "        return src\n",
        "\n",
        "    #complete decoder of transformer\n",
        "    def decode(self, mem, tgt, input_mask=None, target_mask=None, target_padding_mask=None):\n",
        "        tgt = tgt.transpose(0, 1)\n",
        "        tgt = self.word_emb(tgt) * math.sqrt(self.nhid)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        output = self.transformer_decoder(tgt, mem,\n",
        "                                          memory_mask=input_mask,\n",
        "                                          tgt_mask=target_mask,\n",
        "                                          tgt_key_padding_mask=target_padding_mask)\n",
        "        output = self.dec_fc(output)\n",
        "        return output\n",
        "\n",
        "    #main forward pass\n",
        "    def forward(self, src, tgt, input_mask=None, target_mask=None, target_padding_mask=None):\n",
        "        #calling encoder\n",
        "        mem = self.encode(src)\n",
        "        #calling decoder\n",
        "        output = self.decode(mem, tgt,\n",
        "                             input_mask=input_mask,\n",
        "                             target_mask=target_mask,\n",
        "                             target_padding_mask=target_padding_mask)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0QQsahj-Myt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Our Model"
      ],
      "metadata": {
        "id": "MpK4BSh2E1bw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDKFJFqh-Myu",
        "outputId": "90d5aa93-8e07-41d1-f449-696b78de7e49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2021-12-17 at 16:05:37 | Training setting:\n",
            "{'data': {'batch_size': 32,\n",
            "          'input_field_name': 'audio_data',\n",
            "          'load_into_memory': False,\n",
            "          'num_workers': 0,\n",
            "          'type': 'clotho'},\n",
            " 'decoder': {'activation': 'gelu',\n",
            "             'decoder_only': True,\n",
            "             'dim_feedforward': 2048,\n",
            "             'dropout': 0.2,\n",
            "             'nhead': 4,\n",
            "             'nhid': 128,\n",
            "             'nlayers': 2},\n",
            " 'encoder': DotMap(model='Cnn10', freeze=False, pretrained=True),\n",
            " 'finetune': {'audiocap': False,\n",
            "              'epochs': 20,\n",
            "              'lr': 0.0001,\n",
            "              'model': 'pretrained_models/models/submission1/best_model.pt'},\n",
            " 'mode': 'train',\n",
            " 'path': {'audiocaps': {'words_freq': 'audiocaps/pickles/words_freq.p',\n",
            "                        'words_list': 'audiocaps/pickles/words_list.p'},\n",
            "          'clotho': {'words_freq': 'data/pickles/words_freq.p',\n",
            "                     'words_list': 'data/pickles/words_list.p'},\n",
            "          'encoder': 'pretrained_models/encoder/',\n",
            "          'model': 'pretrained_models/models/submission1/best_model.pt',\n",
            "          'word2vec': 'pretrained_models/word2vec/w2v_all_vocabulary.model'},\n",
            " 'rl': {'epochs': 2,\n",
            "        'lr': 0.0001,\n",
            "        'mode': 'greedy',\n",
            "        'model': 'pretrained_models/models/submission1/best_model.pt'},\n",
            " 'test': DotMap(model=''),\n",
            " 'training': {'alpha': 0.2,\n",
            "              'clip_grad': 2,\n",
            "              'epochs': 2,\n",
            "              'keyword': False,\n",
            "              'label_smoothing': True,\n",
            "              'lr': 0.001,\n",
            "              'mixup': False,\n",
            "              'seed': 20,\n",
            "              'spec_augmentation': True},\n",
            " 'wave': {'fmax': 14000,\n",
            "          'fmin': 50,\n",
            "          'hop_length': 512,\n",
            "          'mel_bins': 64,\n",
            "          'sr': 44100,\n",
            "          'window_size': 1024},\n",
            " 'word_embedding': DotMap(pretrained=False, freeze=True)}\n",
            " 2021-12-17 at 16:05:38 | Model:\n",
            "TransformerModel(\n",
            "  (feature_extractor): Cnn10(\n",
            "    (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (spectrogram_extractor): Spectrogram(\n",
            "      (stft): STFT(\n",
            "        (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(512,), bias=False)\n",
            "        (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(512,), bias=False)\n",
            "      )\n",
            "    )\n",
            "    (logmel_extractor): LogmelFilterBank()\n",
            "    (conv_block1): ConvBlock(\n",
            "      (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv_block2): ConvBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv_block3): ConvBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv_block4): ConvBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            "  (pos_encoder): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (audio_linear): AudioLinear(\n",
            "    (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  )\n",
            "  (transformer_decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.2, inplace=False)\n",
            "        (dropout2): Dropout(p=0.2, inplace=False)\n",
            "        (dropout3): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
            "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.2, inplace=False)\n",
            "        (dropout2): Dropout(p=0.2, inplace=False)\n",
            "        (dropout3): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dec_fc): Linear(in_features=128, out_features=6636, bias=True)\n",
            "  (generator): Softmax(dim=-1)\n",
            "  (word_emb): Embedding(6636, 128)\n",
            ")\n",
            "\n",
            " 2021-12-17 at 16:05:38 | Total number of parameters:9122156\n",
            " 2021-12-17 at 16:05:38 | Training mode.\n",
            " 2021-12-17 at 16:05:38 | Training epoch 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|█████▌                                                                             | 1/15 [01:01<14:16, 61.17s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-12-95815a721c35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mmain_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training epoch {epoch}...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[0mmain_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training done.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-12-95815a721c35>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m                          tgt.contiguous().view(-1))\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train():\n",
        "    start_time = time.time()\n",
        "    batch_losses = torch.zeros(len(training_data))\n",
        "    model.train()\n",
        "    for batch_idx, train_batch in tqdm(enumerate(training_data), total=len(training_data)):\n",
        "        src, tgt, f_names, tgt_len, captions = train_batch\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        #masking data\n",
        "        tgt_pad_mask = set_tgt_padding_mask(tgt, tgt_len)\n",
        "        optimizer.zero_grad()\n",
        "        #prediction from model\n",
        "        y_hat = model(src, tgt, target_padding_mask=tgt_pad_mask)\n",
        "        #real target value\n",
        "        tgt = tgt[:, 1:]\n",
        "        y_hat = y_hat.transpose(0, 1)  # batch x words_len x ntokens\n",
        "        y_hat = y_hat[:, :tgt.size()[1], :]\n",
        "        #loss function\n",
        "        loss = criterion(y_hat.contiguous().view(-1, y_hat.size()[-1]),\n",
        "                         tgt.contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.clip_grad)\n",
        "        optimizer.step()\n",
        "        batch_losses[batch_idx] = loss.cpu().item()\n",
        "    \n",
        "    end_time = time.time()\n",
        "    elasped_time = end_time - start_time\n",
        "    epoch_loss = batch_losses.mean()\n",
        "    current_lr = [param_group['lr'] for param_group in optimizer.param_groups][0]\n",
        "    main_logger.info('epoch: {}, train_loss: {:.4f}, time elapsed: {:.4f}, lr:{:02.2e}'.format(epoch, epoch_loss, elasped_time, current_lr))\n",
        "\n",
        "#collecting data from settings file\n",
        "config = get_config()\n",
        "logger.remove()\n",
        "logger.add(sys.stdout, format='{time: YYYY-MM-DD at HH:mm:ss} | {message}',\n",
        "           level='INFO', filter=lambda record: record['extra']['indent'] == 1)\n",
        "main_logger = logger.bind(indent=1)\n",
        "printer = PrettyPrinter()\n",
        "device = \"cpu\"\n",
        "# device_name = (torch.device('cuda'), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "dataset = config.data.type\n",
        "batch_size = config.data.batch_size\n",
        "num_workers = config.data.num_workers\n",
        "input_field_name = config.data.input_field_name\n",
        "\n",
        "# loading vocabulary list\n",
        "words_list_path = 'new_words_list.p'\n",
        "training_data = get_clotho_loader(split='development',\n",
        "                                  input_field_name=input_field_name,\n",
        "                                  load_into_memory=False,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  drop_last=True,\n",
        "                                  num_workers=num_workers)\n",
        "words_list = load_picke_file(words_list_path)\n",
        "ntokens = len(words_list)\n",
        "sos_ind = words_list.index('<sos>')\n",
        "eos_ind = words_list.index('<eos>')\n",
        "\n",
        "#CNN10 modle\n",
        "pretrained_cnn = torch.load(\"Cnn10.pth\",map_location=torch.device('cpu'))\n",
        "main_logger.info('Training setting:\\n'\n",
        "                 f'{printer.pformat(config)}')\n",
        "model = TransformerModel(config, words_list, pretrained_cnn)\n",
        "model.to(device)\n",
        "main_logger.info(f'Model:\\n{model}\\n')\n",
        "main_logger.info('Total number of parameters:'\n",
        "                 f'{sum([i.numel() for i in model.parameters()])}')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "spiders = []\n",
        "\n",
        "main_logger.info('Training mode.')\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=config.training.lr, weight_decay=1e-6)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, 0.1)\n",
        "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=5, after_scheduler=scheduler)\n",
        "epochs = config.training.epochs\n",
        "ep = 1\n",
        "# warm up issue\n",
        "optimizer.zero_grad()\n",
        "optimizer.step()\n",
        "for epoch in range(ep, epochs + 1):\n",
        "    scheduler_warmup.step(epoch)\n",
        "    main_logger.info(f'Training epoch {epoch}...')\n",
        "    train()\n",
        "main_logger.info('Training done.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "audio captioning final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Z2AngCgD-MyF",
        "sOELonNO-udj",
        "cK6UV4Jo_VEL",
        "uuetyY5yAlCu",
        "xK3YGcAaBIzT",
        "QHl0c1kwBcoo",
        "MpK4BSh2E1bw"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}